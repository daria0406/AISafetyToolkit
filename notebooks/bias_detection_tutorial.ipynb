{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Detection Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the AI Safety Toolkit's bias detection module to identify and analyze bias in machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from modules.bias_detection import BiasDetector\n",
    "from modules.data_loader import DataLoader\n",
    "from modules.utils import load_sample_model, generate_synthetic_biased_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Adult Income dataset\n",
    "data_loader = DataLoader()\n",
    "data = data_loader.load_adult_dataset()\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nColumns: {data.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the protected attribute distribution\n",
    "protected_dist = data['protected_race'].value_counts()\n",
    "print(\"Protected attribute distribution:\")\n",
    "print(protected_dist)\n",
    "print(f\"\\nMinority group percentage: {protected_dist[1] / len(data) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for ML\n",
    "X_train, X_test, y_train, y_test, protected_train, protected_test = data_loader.prepare_ml_data(\n",
    "    data, 'income', 'protected_race'\n",
    ")\n",
    "\n",
    "# Train a Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"Model accuracy: {model.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bias Detection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize bias detector\n",
    "bias_detector = BiasDetector()\n",
    "\n",
    "# Calculate key bias metrics\n",
    "disparate_impact = bias_detector.calculate_disparate_impact(y_pred, protected_test)\n",
    "statistical_parity = bias_detector.calculate_statistical_parity(y_pred, protected_test)\n",
    "equalized_odds = bias_detector.calculate_equalized_odds(y_test, y_pred, protected_test)\n",
    "\n",
    "print(\"Bias Detection Results:\")\n",
    "print(f\"Disparate Impact: {disparate_impact:.3f}\")\n",
    "print(f\"Statistical Parity Difference: {statistical_parity:.3f}\")\n",
    "print(f\"Equalized Odds Difference: {equalized_odds:.3f}\")\n",
    "\n",
    "# Interpret results\n",
    "print(\"\\nInterpretation:\")\n",
    "if disparate_impact < 0.8:\n",
    "    print(\"⚠️ Disparate impact violation detected!\")\n",
    "else:\n",
    "    print(\"✅ Disparate impact within acceptable range\")\n",
    "    \n",
    "if abs(statistical_parity) > 0.1:\n",
    "    print(\"⚠️ Statistical parity violation detected!\")\n",
    "else:\n",
    "    print(\"✅ Statistical parity within acceptable range\")\n",
    "    \n",
    "if equalized_odds > 0.1:\n",
    "    print(\"⚠️ Equalized odds violation detected!\")\n",
    "else:\n",
    "    print(\"✅ Equalized odds within acceptable range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comprehensive Bias Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive bias report\n",
    "bias_report = bias_detector.generate_bias_report(y_test, y_pred, protected_test)\n",
    "\n",
    "print(\"Comprehensive Bias Report:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for metric, value in bias_report.items():\n",
    "    if metric != 'interpretations':\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"\\n{metric.upper()}:\")\n",
    "            for sub_metric, sub_value in value.items():\n",
    "                if isinstance(sub_value, (int, float)):\n",
    "                    print(f\"  {sub_metric}: {sub_value:.3f}\")\n",
    "                else:\n",
    "                    print(f\"  {sub_metric}: {sub_value}\")\n",
    "        else:\n",
    "            print(f\"{metric}: {value:.3f}\")\n",
    "\n",
    "print(\"\\nINTERPRETATIONS:\")\n",
    "for interpretation in bias_report['interpretations']:\n",
    "    print(f\"• {interpretation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bias visualization\n",
    "bias_fig = bias_detector.plot_bias_metrics(y_pred, protected_test, y_test)\n",
    "bias_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Synthetic Biased Data Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic biased data with different bias levels\n",
    "bias_levels = [0.0, 0.3, 0.6, 0.9]\n",
    "results = []\n",
    "\n",
    "for bias_level in bias_levels:\n",
    "    # Generate synthetic data\n",
    "    synthetic_data = generate_synthetic_biased_data(n_samples=1000, bias_strength=bias_level)\n",
    "    \n",
    "    # Split data\n",
    "    X = synthetic_data.drop(['target', 'protected_attr'], axis=1)\n",
    "    y = synthetic_data['target']\n",
    "    protected = synthetic_data['protected_attr']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, prot_train, prot_test = train_test_split(\n",
    "        X, y, protected, test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate bias metrics\n",
    "    di = bias_detector.calculate_disparate_impact(y_pred, prot_test)\n",
    "    sp = bias_detector.calculate_statistical_parity(y_pred, prot_test)\n",
    "    \n",
    "    results.append({\n",
    "        'bias_level': bias_level,\n",
    "        'disparate_impact': di,\n",
    "        'statistical_parity': abs(sp)\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Effect of Bias Strength on Fairness Metrics:\")\n",
    "print(results_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the relationship between bias strength and fairness metrics\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Disparate Impact\n",
    "ax1.plot(results_df['bias_level'], results_df['disparate_impact'], 'o-', linewidth=2, markersize=8)\n",
    "ax1.axhline(y=0.8, color='r', linestyle='--', label='Fairness Threshold (0.8)')\n",
    "ax1.set_xlabel('Bias Strength')\n",
    "ax1.set_ylabel('Disparate Impact')\n",
    "ax1.set_title('Disparate Impact vs Bias Strength')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Statistical Parity\n",
    "ax2.plot(results_df['bias_level'], results_df['statistical_parity'], 'o-', linewidth=2, markersize=8, color='orange')\n",
    "ax2.axhline(y=0.1, color='r', linestyle='--', label='Fairness Threshold (0.1)')\n",
    "ax2.set_xlabel('Bias Strength')\n",
    "ax2.set_ylabel('Statistical Parity Difference')\n",
    "ax2.set_title('Statistical Parity vs Bias Strength')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
