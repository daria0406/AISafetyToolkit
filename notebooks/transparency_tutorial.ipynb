{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Transparency Tutorial\n",
    "\n",
    "This notebook demonstrates how to use the AI Safety Toolkit's transparency module to explain model decisions using LIME, SHAP, and other interpretability techniques.\n",
    "\n",
    "### Interpretability Techniques Comparison:\n",
    "\n",
    "| Technique | Best For | Pros | Cons |\n",
    "|-----------|----------|------|------|\n",
    "| **Feature Importance** | Global understanding | Fast, built-in for tree models | Limited to supported models |\n",
    "| **LIME** | Individual predictions | Model-agnostic, intuitive | Can be unstable, local only |\n",
    "| **SHAP** | Both global and local | Theoretically grounded, comprehensive | Computationally expensive |\n",
    "| **Permutation Importance** | Global understanding | Model-agnostic | Computationally expensive |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from modules.transparency import TransparencyAnalyzer\n",
    "from modules.data_loader import DataLoader\n",
    "from modules.utils import load_sample_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Adult Income dataset\n",
    "data_loader = DataLoader()\n",
    "data = data_loader.load_adult_dataset()\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"\\nFeatures: {[col for col in data.columns if col not in ['income', 'protected_race']]}\")\n",
    "print(f\"\\nTarget variable distribution:\")\n",
    "print(data['income'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for ML\n",
    "X_train, X_test, y_train, y_test, protected_train, protected_test = data_loader.prepare_ml_data(\n",
    "    data, 'income', 'protected_race'\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Feature names: {X_train.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Multiple Models for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train different types of models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    \n",
    "    trained_models[name] = model\n",
    "    \n",
    "    print(f\"  Train Accuracy: {train_acc:.3f}\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize transparency analyzer\n",
    "transparency_analyzer = TransparencyAnalyzer()\n",
    "\n",
    "# Analyze feature importance for tree-based models\n",
    "tree_models = ['Random Forest', 'Gradient Boosting']\n",
    "\n",
    "fig, axes = plt.subplots(1, len(tree_models), figsize=(15, 6))\n",
    "if len(tree_models) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, model_name in enumerate(tree_models):\n",
    "    model = trained_models[model_name]\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_names = X_train.columns.tolist()\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    # Sort by importance\n",
    "    indices = np.argsort(importances)[::-1][:10]  # Top 10\n",
    "    \n",
    "    # Plot\n",
    "    axes[i].bar(range(len(indices)), [importances[j] for j in indices])\n",
    "    axes[i].set_title(f'{model_name} - Feature Importance')\n",
    "    axes[i].set_xlabel('Features')\n",
    "    axes[i].set_ylabel('Importance')\n",
    "    axes[i].set_xticks(range(len(indices)))\n",
    "    axes[i].set_xticklabels([feature_names[j] for j in indices], rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive feature importance plot using Plotly\n",
    "model = trained_models['Random Forest']\n",
    "feature_names = X_train.columns.tolist()\n",
    "\n",
    "importance_fig = transparency_analyzer.plot_feature_importance(model, feature_names)\n",
    "importance_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LIME Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few interesting instances to explain\n",
    "model = trained_models['Random Forest']\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Find interesting cases\n",
    "# High confidence positive prediction\n",
    "high_pos_idx = np.argmax(y_proba)\n",
    "# High confidence negative prediction  \n",
    "high_neg_idx = np.argmin(y_proba)\n",
    "# Uncertain prediction (around 0.5)\n",
    "uncertain_idx = np.argmin(np.abs(y_proba - 0.5))\n",
    "\n",
    "interesting_cases = {\n",
    "    'High Positive Confidence': high_pos_idx,\n",
    "    'High Negative Confidence': high_neg_idx,\n",
    "    'Uncertain Prediction': uncertain_idx\n",
    "}\n",
    "\n",
    "print(\"Interesting cases for explanation:\")\n",
    "for case_name, idx in interesting_cases.items():\n",
    "    print(f\"{case_name}: Index {idx}, Probability: {y_proba[idx]:.3f}, Prediction: {y_pred[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LIME explanations for interesting cases\n",
    "for case_name, instance_idx in interesting_cases.items():\n",
    "    print(f\"\\nGenerating LIME explanation for {case_name} (Index: {instance_idx})\")\n",
    "    \n",
    "    try:\n",
    "        lime_fig = transparency_analyzer.explain_with_lime(\n",
    "            model, X_test, instance_idx, num_features=8\n",
    "        )\n",
    "        lime_fig.update_layout(title=f\"LIME Explanation - {case_name}\")\n",
    "        lime_fig.show()\n",
    "        \n",
    "        # Show actual feature values for context\n",
    "        instance_data = X_test.iloc[instance_idx]\n",
    "        print(\"\\nActual feature values:\")\n",
    "        for feature, value in instance_data.items():\n",
    "            print(f\"  {feature}: {value}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating LIME explanation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SHAP Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate SHAP explanations\n",
    "print(\"Generating SHAP analysis...\")\n",
    "print(\"This may take a few moments for complex models.\")\n",
    "\n",
    "try:\n",
    "    # Use a subset of test data for performance\n",
    "    shap_fig = transparency_analyzer.explain_with_shap(\n",
    "        model, X_test.iloc[:100], max_samples=100\n",
    "    )\n",
    "    shap_fig.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error generating SHAP explanation: {e}\")\n",
    "    print(\"Note: SHAP explanations can be computationally intensive for some models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Behavior Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive model behavior analysis\n",
    "behavior_analysis = transparency_analyzer.analyze_model_behavior(\n",
    "    model, X_test, feature_names=X_test.columns.tolist()\n",
    ")\n",
    "\n",
    "print(\"Model Behavior Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Feature importance\n",
    "if 'feature_importance' in behavior_analysis:\n",
    "    print(\"\\nBuilt-in Feature Importance (Top 10):\")\n",
    "    sorted_importance = sorted(behavior_analysis['feature_importance'].items(), \n",
    "                             key=lambda x: x[1], reverse=True)[:10]\n",
    "    for feature, importance in sorted_importance:\n",
    "        print(f\"  {feature}: {importance:.4f}\")\n",
    "\n",
    "# Permutation importance\n",
    "if 'permutation_importance' in behavior_analysis:\n",
    "    print(\"\\nPermutation Importance (Top 10):\")\n",
    "    sorted_perm_importance = sorted(behavior_analysis['permutation_importance'].items(), \n",
    "                                  key=lambda x: x[1], reverse=True)[:10]\n",
    "    for feature, importance in sorted_perm_importance:\n",
    "        print(f\"  {feature}: {importance:.4f}\")\n",
    "\n",
    "# Prediction statistics\n",
    "if 'prediction_stats' in behavior_analysis:\n",
    "    stats = behavior_analysis['prediction_stats']\n",
    "    print(\"\\nPrediction Statistics:\")\n",
    "    print(f\"  Mean: {stats['mean']:.3f}\")\n",
    "    print(f\"  Std: {stats['std']:.3f}\")\n",
    "    print(f\"  Min: {stats['min']:.3f}\")\n",
    "    print(f\"  Max: {stats['max']:.3f}\")\n",
    "    print(f\"  Quartiles: {[f'{q:.3f}' for q in stats['quartiles']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Decision Boundary Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize decision boundary for two most important features\n",
    "if 'feature_importance' in behavior_analysis:\n",
    "    # Get top 2 features\n",
    "    sorted_features = sorted(behavior_analysis['feature_importance'].items(), \n",
    "                           key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    feature1_name = sorted_features[0][0]\n",
    "    feature2_name = sorted_features[1][0]\n",
    "    \n",
    "    feature1_idx = X_test.columns.get_loc(feature1_name)\n",
    "    feature2_idx = X_test.columns.get_loc(feature2_name)\n",
    "    \n",
    "    print(f\"Visualizing decision boundary for {feature1_name} vs {feature2_name}\")\n",
    "    \n",
    "    try:\n",
    "        boundary_fig = transparency_analyzer.plot_decision_boundary_2d(\n",
    "            model, X_test, y_test, feature1_idx, feature2_idx\n",
    "        )\n",
    "        boundary_fig.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating decision boundary plot: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison: Interpretability vs Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare interpretability aspects of different models\n",
    "comparison_results = []\n",
    "\n",
    "for model_name, model in trained_models.items():\n",
    "    result = {\n",
    "        'Model': model_name,\n",
    "        'Test Accuracy': model.score(X_test, y_test),\n",
    "        'Has Feature Importance': hasattr(model, 'feature_importances_'),\n",
    "        'Model Complexity': 'High' if 'Forest' in model_name or 'Boosting' in model_name else 'Low'\n",
    "    }\n",
    "    \n",
    "    # Prediction confidence spread (as proxy for model certainty)\n",
    "    proba = model.predict_proba(X_test)[:, 1]\n",
    "    confidence_spread = np.std(proba)\n",
    "    result['Confidence Spread'] = confidence_spread\n",
    "    \n",
    "    comparison_results.append(result)\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_results)\n",
    "print(\"Model Interpretability Comparison:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracy vs interpretability trade-off\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "ax1.bar(comparison_df['Model'], comparison_df['Test Accuracy'])\n",
    "ax1.set_title('Model Accuracy Comparison')\n",
    "ax1.set_ylabel('Test Accuracy')\n",
    "ax1.set_xticklabels(comparison_df['Model'], rotation=45)\n",
    "\n",
    "# Confidence spread (interpretability proxy)\n",
    "colors = ['green' if x else 'red' for x in comparison_df['Has Feature Importance']]\n",
    "ax2.bar(comparison_df['Model'], comparison_df['Confidence Spread'], color=colors)\n",
    "ax2.set_title('Model Confidence Spread\\n(Green = Has Built-in Feature Importance)')\n",
    "ax2.set_ylabel('Prediction Confidence Spread')\n",
    "ax2.set_xticklabels(comparison_df['Model'], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interactive Interpretability Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive interpretability dashboard\n",
    "try:\n",
    "    dashboard_fig = transparency_analyzer.create_interpretability_dashboard(\n",
    "        model, X_test, y_test, instance_idx=interesting_cases['Uncertain Prediction']\n",
    "    )\n",
    "    dashboard_fig.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error creating interpretability dashboard: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Practical Guidelines for Model Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate practical recommendations based on analysis\n",
    "print(\"Model Interpretability Guidelines:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Model selection recommendations\n",
    "best_accuracy = comparison_df.loc[comparison_df['Test Accuracy'].idxmax()]\n",
    "most_interpretable = comparison_df.loc[comparison_df['Model'] == 'Logistic Regression']\n",
    "\n",
    "print(f\"\\n📊 Best Performance: {best_accuracy['Model']} (Accuracy: {best_accuracy['Test Accuracy']:.3f})\")\n",
    "print(f\"🔍 Most Interpretable: {most_interpretable['Model'].iloc[0]} (Accuracy: {most_interpretable['Test Accuracy'].iloc[0]:.3f})\")\n",
    "\n",
    "# Feature importance insights\n",
    "if 'feature_importance' in behavior_analysis:\n",
    "    top_features = sorted(behavior_analysis['feature_importance'].items(), \n",
    "                         key=lambda x: x[1], reverse=True)[:3]\n",
    "    print(f\"\\n🎯 Top 3 Most Important Features:\")\n",
    "    for i, (feature, importance) in enumerate(top_features, 1):\n",
    "        print(f\"  {i}. {feature} (importance: {importance:.3f})\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\n💡 Recommendations:\")\n",
    "print(\"  • Use LIME for individual prediction explanations\")\n",
    "print(\"  • Use SHAP for global model understanding\")\n",
    "print(\"  • Monitor feature importance drift over time\")\n",
    "print(\"  • Consider simpler models for high-stakes decisions\")\n",
    "print(\"  • Validate explanations with domain experts\")\n",
    "print(\"  • Document model decisions for audit trails\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
